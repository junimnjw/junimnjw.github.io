---
layout: post
title: "Backpropagation, Who are You?"
categories: DEV
date: 2019-09-16
lastmod : 2019-09-16 14:00:00
sitemap :
changefreq : hourly
priority : 1.0
---



<span style="font-size:11pt;color:blue">*오랜만에 네이버 블로그에 작성해 두었던 글들을 재연재하려고 합니다. 지난 블로그 글들을 보니 제가 쓴 글들이지만 허접(?)하네요. 물론 지금 쓰는 글들도 그렇게 고퀄리티는 아니긴 하지만요...그래도 유익하길 바라며, 많은 피드백 부탁드립니다.*</span>



### 들어가며



인공 신경망(Neural Network)에서 **좋은 성능(Performance)**을 얻는데 중요한 몇 가지 요소들이 있습니다. 

* 뛰어난 컴퓨팅 파워(파워풀한 GPU 등, 돈이 필요하네요..)
* 대량의 고품질 Labeled 데이터 (이것도 다 돈이죠.. )
* **학습 알고리듬**

위 요소들 가운데 외부적 요인이 아닌, 소프트웨어 기술과 연관된 것이 바로 **학습 알고리듬**입니다. 즉, 어떻게 하면 <u>컴퓨터(또는 머신)가 더 빠르고 효과적으로 주어진 데이터를 학습할 수 있을까</u>에 대한 고민이죠. 오늘 언급할 내용은 고민에 대한 해법의 근간이 되는 **역전파(Backpropagation) 알고리듬**을 알아보겠습니다. 

<br>



### 본문

 바야흐로 1986년, 제가 엉금엉금 기어다닐 시기에.. 제프리 힌튼(G. Hinton) 교수 등은 인공 신경망 연구에서 획기적인 논문을 발표합니다. 그 논문의 핵심이 바로 **역전파 알고리듬(Backpropagation)**입니다. 이름에서 힌트를 얻을 수 있듯이, 최종 출력층에서 구한 값과 기대값 사이의 차이가 0에 수렴할 때 까지 반복적으로 <span style="color:red">**역-방향**</span>으로 **전파**하는 것이죠. 오차에 대한 전파 순서가 말단층에서 입력층으로 이루어지기 때문에 역전파(Back-propagation)이라고 하죠. 

<br>

개념에 대한 설명은 이정도로 하고, 과정에 대해서 보다 자세히 설명하겠습니다. 아래 그림은 인공 신경망을 도식화 한 것입니다. 쉬운 이해를 위해 총 3개의 층으로 설명하겠습니다. 실제로 딥러닝에서 사용되는 상용 인공 신경망은 수백개의 층과 수백만개의 연결로 이루어져있습니다. 



![img1](/assets/img/backpropagation1.png)



역 전파는 크게 다음과 같이 이루어집니다. 

<br>

1. 순전파
   1. 입력데이터로 부터 실제 출력값 구하기
   2. 실제 출력값과 기대값의 차이, 즉 오차 **E** 구하기
2. 역전파
   1. 출력층에서 입력층 방향으로, 각 연결마다 오차 **E** 대한 기여도 계산
   2. 이렇게 계산한 기여도를 바탕으로 오차가 감소하는 방향으로 해당 연결의 가중치 조정

<br>

 사실 역 전파의 핵심은 2번 항목입니다. 바로 2-1의 **"각 연결마다 오차에 E 대한 기여도"**를 구하는 것, 그리고 2-2의 **"이를 반영한 각 연결의 가중치 조정"**이죠. 

<br>

 우선 1-2의 **"실제 출력값과 기대값의 차이, 즉 오차 E"**에 대한 계산은 다음과 같이 **크로스 엔트로피**식을 사용합니다. (여기서 유도 과정에 대해서는 자세히 다루지 않겠습니다.)
$$
E=-\Sigma({t_klogz_k}+(1-t_k)log(1-z_k))
$$


그리고 최종 말단층(Z)과 그 직전층(Y) 사이의 임의의 연결 \\({W}_{jk}\\)에서 오차에 대한 기여도 계산은,  아래와 같이 정의됩니다. 
$$
\delta E/\delta W_{jk}
$$
오차와 연결 관계에서 특정한 접점에서의 기울기죠. 



## 결론

 이렇게 당대에 획기적이라고 일컬어졌던 **Backpropagation**도 물론 완벽한 알고리듬은 아니었습니다. 인공 신경망의 층이 깊어질수록 학습 데이터에 지나치게 오버피팅(over-fitting)하는 문제가 발견된 것이죠. 가장 큰 원인으로 **그래디언트 소실(Vanishing Gradient Problem)**이 지적되었고, 이러한 제약때문에 1986년부터 2006년까지 다소 긴 시간동안 인공 신경망 연구분야의 빙하기가 있었습니다. 다음 포스트는 이러한 걸림돌이었던 Vanishing Gradient Problem가 도대체 무엇인지 살펴보고, 이를 해결하기 위해 등장한 주요 해결책들을 살펴보는 시간을 가져보도록 하겠습니다.  



## 참고문헌

---
layout: post
title: 'Backpropagation, Who are You?'
categories: DEV
date: 2019-09-16
lastmod : 2019-09-24 05:00:00
sitemap :
changefreq : daily
priority : 1.0
---



<span style="font-size:11pt;color:blue">*오랜만에 네이버 블로그에 작성해 둔 글들을 다시 연재합니다. 지난 글들을 보니 제가 쓴 글이지만 허접(?)하네요. 그래도 유익하길 바라며, 많은 피드백 부탁드립니다.*</span>

### 들어가며

k아래는 신경망(Neural Network) **성능**을 결정짓는데 중요한 요소들입니다. 

* 컴퓨팅 파워(고성능 GPU, 돈이 필요하네요...)
* 학습용 대량의 고품질 Labeled 데이터 (이것도 다 돈이죠...)
* **학습 알고리듬**

 이들 중 소프트웨어 기술과 연관된 것이 **학습 알고리듬**입니다. 즉, 소프트웨어 관점에서 <u>주어진 데이터에 대한 더 빠르고 제대로 된 학습 방법</u>에 대한 부분이죠. 오늘은 이러한 학습 방법 이론의 뼈대인 **역전파(이하 Backpropagation)**를 배워보겠습니다. 

<br>

### 본문

  1986년, 제가 기어다닐 시기에... 제프리 힌튼(G. Hinton) 교수는 신경망 연구에서 획기적인 논문을 발표합니다. 이 논문의 핵심이 **Backpropagation**입니다. 핵심은, 신경망 출력값과 기대값 사이의 오차를 <span style="color:red">**역방향**</span>으로 전파하여 최적의 연결 가중치를 찾는다는 것이죠. 

<br>

 **Backpropagation**의 수학적 원리를 설명하기 위해 2개의 층으로 이루어진 단순한 신경망을 사용하겠습니다. 실제 딥러닝에서 사용되는 신경망들은 훨씬 복잡한 구조를 이루지만, 학습 방법의 기저는 **Backpropagation**에서 출발합니다.  



![img1](/assets/img/backpropagation1.png?style=centerme)



 **Backpropagation**은 신경망 학습에서 층과 층사이를 연결하는 수많은 연결들에 대한 최적의 가중치를 찾는 과정입니다. 최적의 가중치란, 훈련 데이터를 신경망에 넣어서 출력한 값과 기대한 값의 오차 **E**가 최소화 되는 시점에서의 가중치를 의미합니다. 

<br>

 **Backpropagation**과정을 설명하기 전에, 예로 든 신경망에서 두 가지 전제를 두겠습니다. (왜 두가지 전제를 두는지에 대한 대답은 다음기회에...)



1. 활성화 함수는 **시그모이드**를 사용한다.


$$
Z_i=\frac{1}{1+e^{-{S}_{i}}}
$$

$$
where, S_i = \sum_{j=1}{Y_j}{W_{ji}}
$$



2. 오차를 의미하는 비용 함수 **E**는 **크로스엔트로피**를 사용한다


$$
E = -\sum_{i=1}(t_i\log(Z_i)+(1-t_i)\log(1-Z_i))
$$



**Backpropagation**이 이루어지는 과정은 크게 두 단계의 반복입니다. 



1. 순-전파
   1. 입력된 학습데이터로부터의 출력된 값\\(Z\_i\\)과 기대한 값\\(t\_i\\)의 오차 **E** 구하기
2. 역-전파
   1. 오차 **E** 대한 각 연결 별 기여도 계산 (단, 출력층에서 입력층 방향으로!!!)
   2. 해당 그래디언드만큼 연결 가중치 조정
3. E가 최솟값으로 수렴할 때까지 1번과 2번 과정을 반복

<br>

 세부적으로 살펴보죠. 순-전파(Feed-Foward) 부분입니다

1번의 **"실제 출력값과 기대값의 오차 E"**는 다음의 **크로스 엔트로피식**으로 정의합니다. 
$$
E = -\sum_{i=1}(t_i\log(Z_i)+(1-t_i)\log(1-Z_i))
$$

이제 역-전파(Feed-Backward) 부분입니다.

해야할 일은 **"오차 E 대한 각 연결 별 기여도 구하기"**, 그리고 이를 통하여 **"각 연결의 가중치 조정"**입니다. 출력층\\({Z}\_{i}\\)과 그 직전층\\({Y}\_{j}\\)사이의 임의의 연결 \\({W}\_{ji}\\)에서 오차 **E**에 대한 기여도는 아래와 같이 정의됩니다. 
$$
\frac{\partial E}{\partial {W}_{ji}}
$$
즉, 여러 연결들로 구성된 하나의 신경망에서, 오차**E**에 대한 특정 연결의 영향도는 편 미분함을 통해 파악할 수 있는 것입니다. 위 편미분은 **체인 룰**을 이용해서 다음과 같은 유도가 가능합니다. 
$$
\frac{\partial E}{\partial W_{ji}}=\frac{\partial E}{\partial Z_{i}}\frac{\partial Z_i}{\partial S_i}\frac{\partial S_i}{\partial W_{ji}}
$$
다시 우항의 각 부분을 전개하면 다음과 같습니다. 

<br>
$$
\frac{\partial E}{\partial Z_i}=\frac{\partial(-\sum_{i=1}(t_i\log(Z_i)+(1-t_i)\log(1-Z_i)))}{\partial Z_i}=\frac{-t_i}{Z_i}+\frac{1-t_i}{1-Z_i}=\frac{Z_i-t_i}{Z_i(1-Z_i)}
\\
\frac{\partial Z_i}{\partial S_i}=\frac{\partial (\frac{1}{1+e^{-{S}_{i}}})}{\partial S_i}={Z_i}{(1-Z_i)}
\\
\frac{\partial S_i}{\partial W_{ji}}=Y_j
$$

$$
\therefore \frac{\partial E}{\partial W_{ji}}=(Z_i-t_i)\cdot{Y_j}
$$

<br>

그럼 이제 특정 연결 \\({W}\_{ji}\\의 가중치를 조정해볼까요?


$$
W_{ji}={W}_{ji}-{\frac{\partial E}{\partial {W}_{ji}}}
$$




 특정 연결 가중치에서 오차에 기여한 만큼을 감한다는 것을 의미합니다. 이러한 조정을 통해 비용함수 **E**를 최소화 하는 것을 그래디언트 디센트(Graident Descent) 방법이라고 합니다. 지도가 없는 상황에서 어두운 밤 산 중턱에 홀로 남겨졌다고 생각해볼 때, 가장 빨리 마을을 찾는 방법은 내가 위치한 곳에서 <u>급경사를 타고 내려가는 것이죠</u>. **Backpropagation**에서의 그래디언트 디센트를 통한 최적의 연결 가중치를 찾는 것도 같은 과정이라고 볼 수 있습니다.  



#### 그래디언트 소실 문제

 당대에 획기적이라고 언급되었던 **Backpropagation**도 완벽한 알고리듬은 아니었습니다. 인공 신경망의 층이 깊어질수록 학습 시간이 너무 오래걸리거나 학습 데이터에 지나치게 오버피팅(over-fitting)하는 문제가 발견된 것이죠. 가장 큰 원인으로 **그래디언트 소실 문제(Vanishing Gradient Problem)**가 지적되었고, 이 문제점이 1986년부터 2006년까지 신경망 연구분야의 빙하기에 원인 중 하나로 지목되었습니다. 



그래디언트 소실 문제는 왜 발생한 걸까요? 그건 위에서 전제로 했던 활성화 함수를 살펴보면 알 수 있습니다. 위에서 제가 사용했던 활성화 함수는 시그모이드 입니다. 해당 함수의 형태는 다음과 같습니다. 



![시그모이드](/assets/img/sigmoid.jpg)



함수의 형태를 보시면 특징을 금방 파악할 수 있습니다. 바로 함수로 입력된 값이 -1과 1에 가까워질 수록 값이 점점 수렴한다는 것이죠. 즉 기울기가 0에 가까워지게 됩니다. 우리가 위 전개식에서 **Backpropagation** 과정에서 유도한 전개식에서 연결 가중치를 업데이트 하기 위해서 구한 \\(\frac{\partial {Z}\_{i}}{\partial {S}\_{i}}\\\)가 0에 가까워지게 된다는 것을 의미한다. 즉, \\({S}\_{i}\\)가 매우 크거나 작을수록 이에 대한 가중치 업데이트가 너무 느린속도로 진행된다는 것이죠. 특히 이러한 경향은 출력층에서 입력층으로 갈수록 더욱 심각해집니다. 이로 인해 학습 속도가 매우 느리게 되죠. 

 



## 참고문헌

[1]:http://jaejunyoo.blogspot.com/2017/01/backpropagation.html
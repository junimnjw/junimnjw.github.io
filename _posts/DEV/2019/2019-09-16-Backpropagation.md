---
layout: post
title: 'Backpropagation, Who are You?'
categories: DEV
date: 2019-09-16
lastmod : 2019-09-24 05:00:00
sitemap :
changefreq : daily
priority : 1.0
---



<span style="font-size:11pt;color:blue">*오랜만에 네이버 블로그에 작성해 둔 글들을 다시 연재합니다. 지난 글들을 보니 제가 쓴 글이지만 허접(?)하네요. 그래도 유익하길 바라며, 많은 피드백 부탁드립니다.*</span>

<br>

### 들어가며

<br>

 다음은 신경망 **성능**을  결정하는 요소들입니다. 

* 컴퓨터 성능(고성능 GPU, 돈이 필요하네요...)
* 대량의 고품질 학습 데이터 (이것도 다 돈이죠...)
* **학습 알고리듬**

 이들 중 딥러닝에서 중요하지 않은 것이 없습니다. 이 세가지 요소가 들어맞을 때 제대로된 딥러닝 서비스가 가능한 것이죠. 우선은 컴퓨터 성능이나 학습데이터와 같은 외부 요소가 아닌 소프트웨어적으로 <u>주어진 데이터를 더 빠르고 제대로 학습하는 방법</u>, 즉 학습 알고리즘에 대해서 다루고, 그 가운데에서 신경망 학습 알고리즘의 근간이 되는 이론인 **역전파(이하 Backpropagation)**를 알아보도록 하겠습니다.

<br>

### 본문

<br>  1986년, 제가 기어다닐 시기에..., 제프리 힌튼(G. Hinton) 교수는 신경망 연구에서 획기적인 성과를 발표합니다. 바로 **Backpropagation**입니다. 요약하면, 신경망의 출력값과 기대값 사이의 오차에 각 연결들이 미치는 영향도를 계산하고, 이를 가지고 연결 가중치를 조절함으로써 오차를 최소화 해나간다는 논리죠. 이러한 연결 가중치의 갱신 과정이 신경망의 출력층에서 입력층인  <span style="color:red">**역방향**</span>으로 이루어지기에 **Back**-propagation이라고 합니다. 

<br>

 **Backpropagation**의 수학적 원리를 이해하는 것이 가장 올바른 방법이라고 생각합니다. 그래서 원리에 대한 쉬운 설명을 위해서 2개의 층으로 이루어진 단순한 신경망을 사용하겠습니다. 실제 딥러닝에서 사용되는 신경망들은 훨씬 복잡한 구조를 이루지만, 학습 방법의 뿌리가 **Backpropagation**라는 점은 공통적입니다. 

<br>

<center><img src="/assets/img/backpropagation1.png"></center>
<br>

 **Backpropagation**은 신경망 학습에서 층과 층사이를 연결하는 수많은 연결들의 최적의 가중치를 찾는 과정입니다. 과연 최적의 연결 가중치는 어떻게 찾는걸까요? 그건 바로 **최적화 문제**로 접근하는 것입니다. 학습과정에서 신경망에서 출력된 값과 기대한 값 사이의 오차를 **비용함수(이하 Cost Function)**로 두고, 이 함수의 값이 최소가 되도록 하는 함수 파라미터 값(=연결 가중치 값)을 찾는것이죠. 

<br>

이러한 **Cost Function**가 최소(지역적 최소)가 되도록 함수 파라미터들을 조절하는 데는 **경사 하강법(이하 Gradient Descent)** 알고리즘을 사용합니다. 가령, 지도가 없는 상황에서 어두운 밤 산 중턱에 남겨졌다고 할 때, 가장 빨리 마을을 찾는 방법이 무엇일까요? 아마도 현 위치에서 <span style="color:red"><u>급경사를 타고 내려가는 것이죠</u></span>. **Gradient Descent**알고리즘도 **Cost Function**에서 현 위치에서 가장 기울기가 낮은쪽으로 파라미터들의 값을 이동시키고 평평한 지역에 도달할 때까지 이러한 행위를 반복하는 것입니다.   

<br>

 이제 수식으로 **Backpropagation**과정을 살펴보겠습니다. 위에서 제시하였던 2 Layer 신경망을 참고해주세요. 간단한 전제를 두겠습니다. 

1. 신경망에서 활성화 함수는 **시그모이드**를 사용합니다. (논문이 발표된 이래로 가장 많이 사용된 활성화 함수죠.)


$$
Z_i=\frac{1}{1+e^{-{S}_{i}}}
$$

$$
where, S_i = \sum_{j=1}{Y_j}{W_{ji}}
$$

2. 기대값과 출력값 사이의 오차를 의미하는 **Cost Function**은 **크로스엔트로피 식**를 사용합니다.

   <br>

**Backpropagation**은 크게 아래 세 단계로 이루어집니다.   

1. 순-전파 : 학습 데이터로부터 출력된 값\\(Z\_i\\)과 기대한 값\\(t\_i\\)의 오차 **E** 구하기

2. 역-전파 : 출력층에서 입력층 방향으로, 오차 **E** 대한 각 연결 별 기여도 계산하고, 이 기여도만큼 기존 연결 가중치에서 삭감.

3. 전체 연결들의 기여도가 0에 수렴할 때 까지, 또는 그전에 사용자가 지정한 제약조건에 도달할 때까지 1과 2를 반복. 

   <br>

우선, 1번 순-전파(Feed-Foward) 부분입니다.

**"학습데이터로 부터 출력된 값과 기대한 값의 오차 E"**는 다음 **크로스 엔트로피식**으로 정의합니다. 
$$
E = -\sum_{i=1}(t_i\log(Z_i)+(1-t_i)\log(1-Z_i))
$$

다음으로, 2번 역-전파(Feed-Backward) 부분입니다.

오차 E에 대하여, 출력층\\({Z}\_{i}\\)과 그 직전층\\({Y}\_{j}\\)사이의 특정 연결 \\({W}\_{ji}\\)가 **기여하는 정도**는 미분을 사용하고, 아래와 같이 정의됩니다. 

<br>
$$
\frac{\partial E}{\partial {W}_{ji}}
$$
<br>

위 편미분은 다음과 같이 유도가 가능합니다. 

<br>
$$
\frac{\partial E}{\partial W_{ji}}=\frac{\partial E}{\partial Z_{i}}\frac{\partial Z_i}{\partial S_i}\frac{\partial S_i}{\partial W_{ji}}
$$
<br>

다시 우항의 각 부분을 전개하면 다음과 같습니다. 

<br>
$$
\frac{\partial E}{\partial Z_i}=\frac{\partial(-\sum_{i=1}(t_i\log(Z_i)+(1-t_i)\log(1-Z_i)))}{\partial Z_i}=\frac{-t_i}{Z_i}+\frac{1-t_i}{1-Z_i}=\frac{Z_i-t_i}{Z_i(1-Z_i)}
\\
\frac{\partial Z_i}{\partial S_i}=\frac{\partial (\frac{1}{1+e^{-{S}_{i}}})}{\partial S_i}={Z_i}{(1-Z_i)}
\\
\frac{\partial S_i}{\partial W_{ji}}=Y_j
$$

$$
\therefore \frac{\partial E}{\partial W_{ji}}=(Z_i-t_i)\cdot{Y_j}
$$

<br>

그럼 이제 특정 연결 \\({W}\_{ji}\\의 가중치를 조정하는 식을 볼까요? 아래가 갱신된 가중치를 구하는 식입니다. \\(\alpha\\)는 사용자 지정 학습률 파라미터입니다.  


$$
W_{ji}={W}_{ji}-\alpha{\frac{\partial E}{\partial {W}_{ji}}}
$$




 이 식을 해석하면 현재 연결 가중치에서 오차에 기여한 만큼을 감한다는 것을 의미합니다. 이러한 조정을 통해 비용함수를 점차 최소화 시키고, 이를 **Graident Descent**라고 하죠.  

#### 그래디언트 소실 문제

 당대에 획기적이었던 **Backpropagation**도 완벽한 알고리즘은 아니었습니다. 신경망의 층이 깊어질수록 학습 시간이 너무 오래걸리거나 학습 데이터에 지나치게 오버피팅(over-fitting)하는 문제가 발견된 것이죠. 가장 큰 원인으로 **그래디언트 소실 문제(이하 Vanishing Gradient Problem)**가 지목되었고, 이 문제를 해결하기까지 1986년부터 2006년까지 신경망 연구분야에 침체기가 있었습니다. 

 Vanishing Gradient Problem는 사실은 신경망에서 사용했던 시그모이드라는 활성화 함수 때문에 발생하였습니다. Backpropagation이 제안되었을 때 당시만 하더라도 활성화 함수로 비선형인 시그모이드 함수를 사용하고 있었죠.  



시그모이드는 non-linear 함수이기 때문에 모든 실수 세계의 값을 0과 1사이로 압축시켜주는 녀석입니다. 하지만 다음 차트의 오른쪽 부분인 시그모이드 함수의 미분상태를 보면 입력된 값이 0 주변인 경우를 제외하고는 대부분은 0에 가까운 값들을 가집니다. 이러한 특성때문에 신경망의 층이 깊어질 수록 **Backpropagation** 학습 알고리즘을 적용했을 때, 출력층에서 입력층으로 가는 과정에서 시그모이드 함수의 미분값이 워낙 미미해지고 옅어지기때문에 사실상 입력층에 가까워지면 연결 가중치에 대한 업데이트가 발생하지 않게 되는것이죠. 그리고 이러한 문제로 학습이 제대로 이루어지지 못한 상태가 발생하게 됩니다. 

![시그모이드](/assets/img/sigmoid.png)

그나마 출력층에서는 괜찮은데, 왜 입력층으로 갈 수록 연결 가중치에 대한 업데이트가 제대로 이루어지지 않는 걸까요? 수식으로 한번 보겠습니다. 

 



## 참고문헌

[1]:http://jaejunyoo.blogspot.com/2017/01/backpropagation.html
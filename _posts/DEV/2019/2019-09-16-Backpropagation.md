---
layout: post
title: "Backpropagation, Who are You?"
categories: DEV
date: 2019-09-16
lastmod : 2019-09-16 14:00:00
sitemap :
changefreq : hourly
priority : 1.0
---



<span style="font-size:11pt;color:blue">*오랜만에 네이버 블로그에 작성해 두었던 글들을 재연재하려고 합니다. 지난 블로그 글들을 보니 제가 쓴 글들이지만 허접(?)하네요. 물론 지금 쓰는 글들도 그렇게 고퀄리티는 아니긴 하지만요...그래도 유익하길 바라며, 많은 피드백 부탁드립니다.*</span>



 인공 신경망에서 **좋은 성능(Performance)**을 얻는데 중요한 몇 가지 요소들이 있습니다. 

* 뛰어난 컴퓨팅 파워(파워풀한 GPU 등, 돈이 필요하네요..)
* 대량의 고품질 Labeled 데이터 (이것도 다 돈이죠.. )
* **학습 알고리듬**

위 요소들 가운데 외부적 요인이 아닌, 그나마 기술과 직접 연관된 것이 바로 **학습 알고리듬** 입니다. 즉, 어떻게 하면 <u>컴퓨터(또는 머신)가 더 빠르고 효과적으로 주어진 데이터들을 학습할 수 있을까</u>에 대한 고민이죠. 오늘 언급할 내용은 그 고민에 대한 해법의 근간이 되는 **역전파(Backpropagation) 알고리듬**을 알아보겠습니다. 



## 본문

 바야흐로 1986년, 제가 엉금엉금 기어다닐 시기에.. 제프리 힌튼(G. Hinton) 교수 등은 인공 신경망 연구에서 획기적인 논문을 발표합니다. 그 논문의 핵심이 바로 **역전파 알고리듬(Backpropagation)**입니다. 



 역전파(?)의 개념은 이름에서 힌트를 얻을 수 있습니다. 인공 신경망에서 **지도학습(Supervised-Learning)**과정은, 학습용 데이터를 입력으로 넣고, 여러 층을 거쳐 말단 층에서 출력된 값과 입력된 데이터의 기대값(레이블)이 다른 경우 오차(Error)로 규정합니다. 이러한 과정을 순-방향이라고 할 때,  이 오차에 대한 역-방향으로 전파하는 것이죠. 오차에 대한 전파 순서가 말단층에서 입력층으로 이루어지기 때문에 역전파(Back-propagation)이라고 하죠. 



 개념에 대한 설명은 이정도로 하고, 그 과정에 대해서 보다 자세히 설명하겠습니다. 

역 전파가 과정은 다음과 같이 이루어집니다. 

* 순전파
  * 예측을 통한 실제 출력값 구하기
  * 실제 출력값과 기대값의 차이, 즉 오차 구하기
* 역전파
  * 출력층에서 입력층 방향으로, 각 연결의 오차에 대한 기여도 계산
  * 오차가 감소하는 방향으로 가중치 조정



**각 연결의 오차에 대한 기여도 계산**

**오차가 감소하는 방향으로 가중치 조정**

이 두가지가 이론의 핵심입니다. 



우선 오차(Error)의 정의는 다음과 같이 크로스 엔트로피 식을 사용합니다. 
$$
E=-\Sigma({t_klogz_k}+(1-t_k)log(1-z_k))
$$


각 연결의 오차에 대한 기여도는 결국 편미분인 아래와 같이 정의됩니다. 
$$
\delta E/\delta W_{jk}
$$
오차와 연결 관계에서 특정한 접점에서의 기울기죠. 



![img1](/assets/img/backpropagation1.png)



## 결론

ㅇㄹ
dddd





## 참고문헌

---
layout: post
title: "Tensorflow Serving, Who are You?"
categories: DEV
date: 2019-08-22
lastmod : 2019-08-22 14:00:00
sitemap :
changefreq : hourly
priority : 1.0
---



안녕하세요. 코딩벅스입니다.   



## 도입

 단순히 개인 연구 목적으로 텐서플로우를 사용하는 분들께는 이번 포스트가 큰 관심 거리가 되지 않을 수도 있습니다. 왜냐하면 이번 포스트에서 다루는 내용은 텐서플로우 모델을 어떻게 하면 쉽게 **배포**하고 **서비스**할지에 대한 것이니까요. 텐서플로우 모델을 **배포(또는 서빙)** 한다는 건 단순한 문제는 아닙니다. 여러 **고객(Client)**들의 요청에 대한 **스케쥴링**도 필요하고, 중간 중간 업데이트된 딥러닝 모델이 **매끄럽게(Seamless)** 배포되도록 신경써야하는 등, 원활한 서비스 제공을 위해서는 여러가지를 고민하고 이를 구현해야하는 것이죠. 텐서플로우를 만든 구글(Google)은 이러한 텐서플로우 사용자의 고민거리를 덜어주기 위해 텐서플로우로 학습한 딥러닝 모델을 제품환경에서의 보다 쉽게 **배포**할 수 있도록 도와주는 별도의 프레임워크를 제공하고있고, 이를 **텐서플로우 서빙(Tensorflow Serving)**라고 합니다. 이번 포스트는 텐서플로우 서빙과 관련해서 잘 설명된 [관련 블로그][1]를 간략히 요약한 것임을 알려드립니다.



## 본문



#### 구조

텐서플로우 서빙의 구조는 직관적으로, 다음과 같이 크게 네가지 컴포넌트들로 구성됩니다. 

* Source
  * TF(Tensorflow) 모델에 대한 지속적인 모니터링 및 검증, 이후 Loader 생성
* Loader
  * 특정 딥러닝 모델에 대한 모든 정보를 가짐. 로드에 필요한 리소스 정보도 포함함. 
* 
* Manager
* ServableHandle

<img src="https://cdn-media-1.freecodecamp.org/images/1*TwfOoS3M8DaUiB7ntP07_w.png" alt="img" style="zoom:80%;" />



#### LifeCycle

* 우선, **Source**가 파일 시스템에서 지속적인 모니터링을 통해서 딥러닝 모델 파일을 탐색하고, 발견시 이를 검증합니다. 검증이 끝나면, **Loader**를 생성합니다. 

* **Source**가 **Loader**를 생성한 후, 해당 **Loader**를 **DynamicManager(이하 Manager)**에게 전달합니다. 

* **Manager**는 **Loader**를 **Source**로 부터 전달받자마자 **서빙 프로세스**를 시작합니다. 여기서 두가지 경우를 고려할 수 있습니다. 

  * 첫번째 경우는, 가장 처음으로 모델에 대한 로드가 이루어지는 경우입니다. 이때는 **Loader**를 통해서 확인한 딥러닝 모델 서빙에 필요한 가용 리소스를 확인 후 문제없으면 **Loader**에게 딥러닝 모델을 로드하도록 권한을 줍니다. 
  * 두번째 경우는, 이미 한가지 모델이 로드된 상황에서, 새로운 버젼의 모델에 대한 서빙 요청이 들어오는 경우입니다. 이때 **DynamicManager**는 신규 딥러닝 모델에 대한 로드시 두가지 기준을 두고 우선순위를 선택할 수 있도록 합니다. 첫번째는 **이용도(Availability)**, 그리고 두번째는 **리소스(Resource)**입니다. 이용도가 중요한 부분으로 선택한다면, 상위 버전이 모델에 대한 로드가 이루어지기는 하지만, 기존 이전 버전의 모델에 대한 고객의 요청이 끝나는 시점까지는 두가지 모델을 동시에 유지하게 됩니다. **리소스**를 중요한 부분으로 선택하게 된다면, 위 처럼 두가지 모델을 동시에 유지하지 않고, 리소스 절약 관점에서 하나만 유지하도록 선택하게 됩니다. 규모가 큰 모델의 경우에는 이러한 선택을 하는게 이용도(Availability)에서는 약간의 손해를 보더라도 좋은 선택일 수 있습니다. 

* 이렇게 **DynamicManager**는 **Loader**에게 딥러닝 모델을 로드하도록 권한을 부여하고, 해당 **Loader**에 대한 핸들을 **Serverable**에게 전달합니다. 

  

  

​	



**Source Class**는 딥러닝 모델 파일을 읽어 들이고, 올바른 형태인지 검증하는 컴포넌트입니다. 올바른 포맷으로 저장된 모델임을 확인한 후, Loader 객체를 생성하고 Loader에게 해당 딥러닝 모델을 넘깁니다. 

**Loader Class**는 딥러닝 모델에 대한 정보를 가지고 있는 객체입니다. 필요한 리소스(모델 사이즈, 필요한 메모리 등)에 대한 정보는 물론이고, 모델에 대한 메타 그래프와 가중치를 포함한 전반적인 정보를 가지게 됩니다. 다만, 이러한 정보를 가지게 되는 시점은 Loader가 해당 모델을 로드하고 난 이후인데, Loader는 스스로 주어진 딥러닝 모델을 로드하는 것이 아니고, Manager의 통제에 의해서만 가능합니다.

**Manager Class**는 프로세스입니다. 하나의 모델에 대해서 프로세스를 할당하고, Loader가 주어진 딥러닝 모델을 로드할 수 있도록 허락합니다.  



#### Tensorflow Serving & SavedModel

텐서플로우 모델을  ***Serve*** 하기 위해서는 **SavedModel**로 저장해야합니다. 

**SavedModel**은 이전 포스트에서 자세히 다루었으니 참고해주세요. 

## 결론

여기까지, 텐서플로우 서빙이 무엇인지 간략히 알아보았습니다. 딥러닝 모델에 대한 실제 양산화를 생각하고 계신 분들은 틈틈이 배워 놓는 것도 도움이 될 것입니다. 감사합니다. 

## 참고문헌

[1]:https://www.freecodecamp.org/news/how-to-deploy-tensorflow-models-to-production-using-tf-serving-4b4b78d41700/


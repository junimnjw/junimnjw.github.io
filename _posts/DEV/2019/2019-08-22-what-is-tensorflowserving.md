---
layout: post
title: "Tensorflow Serving, Who are You?"
categories: DEV
date: 2019-08-22
lastmod : 2019-08-22 14:00:00
sitemap :
changefreq : hourly
priority : 1.0
---



안녕하세요. 코딩벅스입니다.   



## 도입

  단순히 개인 연구 목적으로 텐서플로우를 사용하는 분들께는 이번 포스트가 큰 관심 거리가 되지 않을 수도 있습니다. 왜냐하면 이번 포스트에서 다루는 내용은 텐서플로우 모델을 어떻게 하면 쉽게 **배포**하고 **서비스**할지에 대한 것이니까요. 텐서플로우 모델을 **배포(또는 서빙)** 한다는 건 그리 단순한 문제는 아닙니다. 여러 **고객(Client)**들의 요청에 대한 **스케쥴링**도 필요하고, 서비스 중간 중간 업데이트된 딥러닝 모델이 **매끄럽게(Seamless)** 적용되도록 신경써야하는 등, 여러가지를 고민하고 이를 구현해야하기 때문이죠. 텐서플로우를 만든 구글(Google)은 이러한 텐서플로우 사용자들의 고민거리를 덜어주기 위해 텐서플로우로 학습한 딥러닝 모델을 제품환경에서 보다 쉽게 **배포하고 서비스**할 수 있도록 별도의 프레임워크를 제공하고있고, 이를 **텐서플로우 서빙(Tensorflow Serving)**라고 합니다. 이번 포스트는 텐서플로우 서빙이 무엇이고 어떻게 사용하는지에 대한 설명을 하고자 합니다. 본 포스트는 [관련 블로그][1]를 간략히 요약한 것임을 알려드립니다.



## 본문

#### 구조

텐서플로우 서빙의 구조는 크게 다음과 같이 네가지 모듈로 구성됩니다. 

* Source
* Loader 
* DynamicManager
* ServableHandle

<img src="https://cdn-media-1.freecodecamp.org/images/1*TwfOoS3M8DaUiB7ntP07_w.png" alt="img" style="zoom:80%;" />

####  동작흐름

* 우선, **Source**가 파일 시스템에서 지속적인 모니터링을 통해서 딥러닝 모델을 탐색하고, 발견시 이를 검증합니다. 검증을 통해서 딥러닝 모델로써 로딩이 가능하다는 판단이 나면, **Loader**를 생성합니다. 

* **Source**가 **Loader**를 생성한 후, 해당 **Loader**를 **DynamicManager(이하 Manager)**에게 전달합니다. 

* **Manager**는 전달받은 **Loader**를 가지고 **서빙 프로세스**를 시작합니다. 여기서 다음 두가지 경우가 발생하게 됩니다. 

  * 최초 로드: 가장 처음으로 모델에 대한 로드가 이루어지는 경우입니다. 이때는 **Loader**를 통해서 확인한 딥러닝 모델 서빙에 필요한 가용 리소스를 확인 후 문제없으면 **Loader**에게 딥러닝 모델을 로드하도록 권한을 줍니다. 
  * 이후 로드: 이미 한가지 모델이 로드되어있는 상황에서,  **Source**로 부터 새로운 버전의 딥러닝 모델에 대한 **Loader**를 전달받는 경우죠. 이때 **Manager**는 두가지 기준을 두고 이후 행동을 선택할 수 있도록 합니다. 첫번째는 **이용도(Availability)**, 그리고 두번째는 **리소스(Resource)**입니다. **이용도**를 중요한 부분으로 선택하면, 상위 버전에 대한 모델 로드가 이루어지기는 하지만, 기존 버전의 모델에 대한 작업이 진행 중이었다면 끝나는 시점까지는 두가지 모델을 동시에 유지합니다. 다음으로 **리소스**를 중요한 부분으로 선택하게 된다면, 위 처럼 두가지 모델을 동시에 유지하지 않고, 리소스를 절약하도록 하나의 모델만 유지하도록 선택하게 됩니다. 사이즈가 큰 딥러닝 모델의 경우에는 이러한 선택을 하는게 이용도(Availability)에서 다소 손해를 보더라도 좋은 선택이 될 수 있습니다. 

* 다음으로 **Manager**는 **Loader**에게 자신이 가지고 있는 대상 딥러닝 모델을 로드할 수 있도록 허락하고, 해당 **Loader**에 대한 핸들을 **Serverable**에게 전달합니다. 

  

  

#### Tensorflow Serving & SavedModel

텐서플로우 모델을  ***Serve*** 하기 위해서는 **SavedModel**로 저장해야합니다. 

**SavedModel**은 이전 포스트에서 자세히 다루었으니 참고해주세요. 

## 결론

여기까지, 텐서플로우 서빙이 무엇인지 간략히 알아보았습니다. 딥러닝 모델에 대한 실제 양산화를 생각하고 계신 분들은 틈틈이 배워 놓는 것도 도움이 될 것입니다. 감사합니다. 

## 참고문헌

[1]:https://www.freecodecamp.org/news/how-to-deploy-tensorflow-models-to-production-using-tf-serving-4b4b78d41700/


---
layout: post
title: "Tensorflow Serving, Who are You?"
categories: DEV
date: 2019-08-22
lastmod : 2019-09-05 14:00:00
sitemap :
changefreq : hourly
priority : 1.0
---



안녕하세요. 코딩벅스입니다.   



## 도입

  단순히 개인 연구 목적으로 텐서플로우를 사용하는 분들께는 이번 포스트가 큰 관심 거리가 되지 않을 수도 있습니다. 왜냐하면 이번 포스트에서 다루는 내용은 텐서플로우 모델을 어떻게 하면 쉽게 **배포**하고 **서비스**할지에 대한 것이니까요. 내가 만든 텐서플로우 모델을 **배포하고 서비스** 한다는 건 단순한 문제는 아닙니다. 여러 **고객(Client)**들의 사용 요청에 대한 **스케쥴링**도 필요하고, 서비스 중간에 업데이트한 신규 버전의 딥러닝 모델도 **매끄럽게(Seamless)** 적용되게 해야하는 등, 여러가지를 고민하고 이를 구현해야하기 때문이죠. 텐서플로우를 만든 구글(Google)은 이러한 텐서플로우 사용자들의 고민을 덜어주기위해 텐서플로우로 학습한 머신러닝 모델을 운영환경에서 보다 쉽게 **배포하고 서비스**할 수 있도록 별도의 라이브러리를 제공하고, 이를 **텐서플로우 서빙(이하 TF Serving)**라고 합니다. 이번 포스트는 이러한 텐서플로우 서빙의 간략한 구조와 동작원리, 그리고 어떻게 **실제** 사용하는지에 대한 설명을 하고자 합니다. 본 포스트는 [관련 블로그][1]를 간략히 요약한 것임을 알려드립니다.



## 본문

#### 구조

텐서플로우 서빙의 구조는 크게 다음과 같이 네가지 모듈로 구성됩니다. 

* Source
* Loader 
* DynamicManager
* ServableHandle

<img src="https://cdn-media-1.freecodecamp.org/images/1*TwfOoS3M8DaUiB7ntP07_w.png" alt="img" style="zoom:80%;" />

####  흐름

* **Source**는 운영환경의 파일 시스템에서 지속적인 모니터링을 통해 머신러닝 모델을 탐색하고, 이를 검증하는 역할을 담당합니다. 그리고 해당 머신러닝 모델에 대한 검증이 통과되면, 이 모델에 대한 **Loader**를 생성합니다. 

* **Loader**는 주어진 머신러닝 모델에 대한 모든 정보를 가지고 있습니다. 하지만 본인 스스로 모델에 대한 정보를 로딩하지는 않고, 이후의 Manager에 의해 통제받습니다. 

* **Source**가 **Loader**를 생성한 후, 해당 **Loader**를 **DynamicManager(이하 Manager)**에게 전달합니다. 

* **Manager**는 전달받은 **Loader**를 가지고 **서빙 프로세스**를 시작합니다. 여기서 다음의 두가지 경우가 발생할 수 있습니다. 

  * 최초 로드: 맨 처음 머신러닝 모델에 대한 로드가 이루어지는 상황입니다. 이때는 **Loader**를 통해서 머신러닝 모델 서빙에 필요한 리소스를 확인하고, 현재 보유한 리소스에 대해서 문제없으면 **Loader**가 모델을 로드하도록 권한을 줍니다. 
  * 이후 로드: 이미 기존 모델이 서비스 되는 상황에서,  **Source**로 부터 상위 버전 모델이 발견되었다는 소식을 받는 경우죠. 이때 **Manager**는 이후 행동을 결정짓는데 두가지 기준을 두고있습니다. 첫번째는 **이용도(Availability)**이고, 두번째는 **리소스(Resource)**입니다. **이용도**를 중요시하겠다면, 상위 버전 모델에 대한 로드가 완전히 이루어진 상태에서만, 이전 버전 모델을 해제하죠. 그런데 이렇게 대면 순간적으로 두가지 모델이 동시에 로드된 상태가 발생할 수 밖에 없습니다. 그래서 서비스 측면에서 흐름이 끊기지 않고 안전한 면이 있지만 추가적인 리소스가 필요하게 되죠. 만약에 **리소스**항목을 보다 중요한 부분으로 선택하게 된다면, 기존 모델에 대한 해제 이후, 새로운 모델을 로드함을 통해 리소스를 절약하게 됩니다. 운영환경의 리소스에 제약이 있고 대상 머신러닝 모델의 사이즈가 큰 경우 사용자의 이용도(Availability)측면에서 약간 손해를 보더라도 이러한 경우가 좋은 선택이 될 수 있습니다. 

* 다음으로 **Manager**는 **Loader**에게 자신이 가지고 있는 대상 딥러닝 모델을 로드할 수 있도록 허락하고, 해당 **Loader**에 대한 핸들을 **Serverable**에게 전달합니다. 




#### SavedModel과 TF Serving

갑자기 왠 **SavedModel**? 그건 **TF Serving**에서 머신러닝 모델을 동작시키기 위한 지원 포맷이 **SavedModel**이기 때문입니다. 즉, 여러분께서 열심히 학습시킨 머신러닝 모델을 **TF Serving**를 통해 배포하시려면, <u>**SavedModel**</u>로 저장하셔야 합니다. 



#### 나도 도전!

지금부터 실제로 텐서플로우 서빙을 사용해보도록 하죠! 



1. **SavedModel** 생성 

배포할 대상 모델이 필요하겠죠? 여기서는 **Tensorflow**의 **HelloWorld**인 ***MNIST 모델***로 하겠습니다. 텐서플로우 모델을  ***TF Serving***에서 이용할 수 있도록 하기 위해서는 **SavedModel**로 저장해야합니다. **SavedModel**에 관한 자세한 내용은 [이전 포스트][https://junimnjw.github.io/dev/2019/08/14/what-is-saved-model.html]를 참고해주세요. 실습의 속도를 내기위해 언급한 MNIST에 데이터 셋에 대한 머신러닝 모델 생성 및 학습코드, 그리고 **SavedModel Format**으로 Export하는 풀코드는 제 깃헙에 올려두었으니 참고해주세요. 참고로, **SaveModel Format**으로 머신러닝 모델을 저장하기 위해서는, TF Serving 에서 제공하는 API인 <u>**SavedModelBuilder**</u>를 사용하면 됩니다. 해당 클래스의 자세한 사용법은 추후 기회가 되면, 별도로 포스팅 하겠습니다. 



1. 서버 구동 



위에서 저장한 모델을 가지고 운영환경에서 **TF Serving**을 구동하는데는 아래 명령어가 전부입니다. 

~~~
$ tensorflow_model_server --port=9000 --model_name=deeplab --model_base_path=<full/path/to/serving/versions/>
~~~

model_base_path에 이전에 생성해두었던 **SavedModel**의 위치만 입력해주는게 끝입니다. 어떤 버전을 어떻게 로드하는지 어떻게 아는건가요? 걱정마세요. **TF Serving**이 해당 Path내에 존재하는 모델들의 버전 정보를 보고 제일 상위버전을 알아서 로드해줍니다. 



3. 클라이언트 요청부

서버에 수기로 작성한 숫자 이미지 하나(물론 MNIST에서 제공하는 TEST이미지)를 보내고, 이것에 대한 결과를 받는 요청을 작성하겠습니다. 





## 결론

여기까지, 텐서플로우 서빙이 무엇인지 간략히 알아보았습니다. 딥러닝 모델에 대한 실제 양산화를 생각하고 계신 분들은 틈틈이 배워 놓는 것도 도움이 될 것입니다. 감사합니다. 



## 참고문헌

[1]:https://www.freecodecamp.org/news/how-to-deploy-tensorflow-models-to-production-using-tf-serving-4b4b78d41700/	"How to deploy a Tensorlofw Model"

